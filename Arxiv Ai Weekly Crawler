# arxiv_ai_weekly_crawler.py

import requests
import feedparser
import pandas as pd
from datetime import datetime, timedelta

# -------------------- é…ç½®å‚æ•° --------------------
CATEGORIES = ["cs.AI", "cs.CL", "cs.LG"]  # äººå·¥æ™ºèƒ½ã€è®¡ç®—è¯­è¨€å­¦ã€æœºå™¨å­¦ä¹ ç­‰
MAX_RESULTS = 100
DAYS_RANGE = 7  # æœ€è¿‘7å¤©çš„è®ºæ–‡
OUTPUT_FILE = "arxiv_ai_weekly.csv"

# -------------------- ç”ŸæˆæŸ¥è¯¢åŒºé—´ --------------------
today = datetime.utcnow()
start_date = (today - timedelta(days=DAYS_RANGE)).strftime("%Y%m%d0000")
end_date = today.strftime("%Y%m%d2359")

# -------------------- æ„å»ºæœç´¢æŸ¥è¯¢ --------------------
base_url = "http://export.arxiv.org/api/query?"
search_query = "cat:" + " OR cat:".join(CATEGORIES)
url = f"{base_url}search_query={search_query}&start=0&max_results={MAX_RESULTS}&sortBy=submittedDate&sortOrder=descending"

# -------------------- è¯·æ±‚å¹¶è§£æç»“æœ --------------------
print("ğŸš€ æ­£åœ¨æŠ“å– arXiv è®ºæ–‡...")
response = requests.get(url)
feed = feedparser.parse(response.text)

# -------------------- æå–å­—æ®µ --------------------
data = []
for entry in feed.entries:
    published = datetime.strptime(entry.published, "%Y-%m-%dT%H:%M:%SZ")
    if published < today - timedelta(days=DAYS_RANGE):
        continue  # åªä¿ç•™è¿‘7å¤©çš„

    data.append({
        "Title": entry.title.strip().replace("\n", " "),
        "Authors": ", ".join(author.name for author in entry.authors),
        "Published": published.strftime("%Y-%m-%d"),
        "Summary": entry.summary.strip().replace("\n", " "),
        "Category": entry.tags[0]['term'] if entry.tags else "",
        "Link": entry.link
    })

# -------------------- ä¿å­˜ä¸º CSV --------------------
df = pd.DataFrame(data)
df.to_csv(OUTPUT_FILE, index=False)
print(f"âœ… å…±æŠ“å–åˆ° {len(df)} ç¯‡è®ºæ–‡ï¼Œå·²ä¿å­˜è‡³ {OUTPUT_FILE}")
